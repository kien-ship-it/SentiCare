# -*- coding: utf-8 -*-
"""Hackathon.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RapDgiVHwtRyHDorMCFnv8E7cw4dOz9v
"""

import tensorflow as tf
import tensorflow_hub as hub
from tensorflow_docs.vis import embed
import numpy as np
import cv2

# Import matplotlib libraries
from matplotlib import pyplot as plt
from matplotlib.collections import LineCollection
import matplotlib.patches as patches
import matplotlib

# Some modules to display an animation using imageio.
import imageio
from IPython.display import HTML, display

print(tf.config.list_physical_devices('GPU'))

model_name = "movenet_lightning"

if "tflite" in model_name:
  if "movenet_lightning_f16" in model_name:
    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/lightning/tflite/float16/4?lite-format=tflite
    input_size = 192
  elif "movenet_thunder_f16" in model_name:
    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/thunder/tflite/float16/4?lite-format=tflite
    input_size = 256
  elif "movenet_lightning_int8" in model_name:
    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/lightning/tflite/int8/4?lite-format=tflite
    input_size = 192
  elif "movenet_thunder_int8" in model_name:
    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/thunder/tflite/int8/4?lite-format=tflite
    input_size = 256
  else:
    raise ValueError("Unsupported model name: %s" % model_name)

  # Initialize the TFLite interpreter
  interpreter = tf.lite.Interpreter(model_path="model.tflite")
  interpreter.allocate_tensors()

  def movenet(input_image):
    """Runs detection on an input image.

    Args:
      input_image: A [1, height, width, 3] tensor represents the input image
        pixels. Note that the height/width should already be resized and match the
        expected input resolution of the model before passing into this function.

    Returns:
      A [1, 1, 17, 3] float numpy array representing the predicted keypoint
      coordinates and scores.
    """
    # TF Lite format expects tensor type of uint8.
    input_image = tf.cast(input_image, dtype=tf.uint8)
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()
    interpreter.set_tensor(input_details[0]['index'], input_image.numpy())
    # Invoke inference.
    interpreter.invoke()
    # Get the model prediction.
    keypoints_with_scores = interpreter.get_tensor(output_details[0]['index'])
    return keypoints_with_scores

else:
  if "movenet_lightning" in model_name:
    module = hub.load("https://tfhub.dev/google/movenet/singlepose/lightning/4")
    input_size = 192
  elif "movenet_thunder" in model_name:
    module = hub.load("https://tfhub.dev/google/movenet/singlepose/thunder/4")
    input_size = 256
  else:
    raise ValueError("Unsupported model name: %s" % model_name)

  def movenet(input_image):
    """Runs detection on an input image.

    Args:
      input_image: A [1, height, width, 3] tensor represents the input image
        pixels. Note that the height/width should already be resized and match the
        expected input resolution of the model before passing into this function.

    Returns:
      A [1, 1, 17, 3] float numpy array representing the predicted keypoint
      coordinates and scores.
    """
    model = module.signatures['serving_default']

    # SavedModel format expects tensor type of int32.
    input_image = tf.cast(input_image, dtype=tf.int32)
    # Run model inference.
    outputs = model(input_image)
    # Output is a [1, 1, 17, 3] tensor.
    keypoints_with_scores = outputs['output_0'].numpy()
    return keypoints_with_scores

# Load the input image.
image_path = 'input_image.jpeg'
image = tf.io.read_file(image_path)
image = tf.image.decode_jpeg(image)

# Dictionary that maps from joint names to keypoint indices.
KEYPOINT_DICT = {
    'nose': 0,
    'left_eye': 1,
    'right_eye': 2,
    'left_ear': 3,
    'right_ear': 4,
    'left_shoulder': 5,
    'right_shoulder': 6,
    'left_elbow': 7,
    'right_elbow': 8,
    'left_wrist': 9,
    'right_wrist': 10,
    'left_hip': 11,
    'right_hip': 12,
    'left_knee': 13,
    'right_knee': 14,
    'left_ankle': 15,
    'right_ankle': 16
}

# Maps bones to a matplotlib color name.
KEYPOINT_EDGE_INDS_TO_COLOR = {
    (0, 1): 'm',
    (0, 2): 'c',
    (1, 3): 'm',
    (2, 4): 'c',
    (0, 5): 'm',
    (0, 6): 'c',
    (5, 7): 'm',
    (7, 9): 'm',
    (6, 8): 'c',
    (8, 10): 'c',
    (5, 6): 'y',
    (5, 11): 'm',
    (6, 12): 'c',
    (11, 12): 'y',
    (11, 13): 'm',
    (13, 15): 'm',
    (12, 14): 'c',
    (14, 16): 'c'
}

def _keypoints_and_edges_for_display(keypoints_with_scores,
                                     height,
                                     width,
                                     keypoint_threshold=0.11):
  """Returns high confidence keypoints and edges for visualization.

  Args:
    keypoints_with_scores: A numpy array with shape [1, 1, 17, 3] representing
      the keypoint coordinates and scores returned from the MoveNet model.
    height: height of the image in pixels.
    width: width of the image in pixels.
    keypoint_threshold: minimum confidence score for a keypoint to be
      visualized.

  Returns:
    A (keypoints_xy, edges_xy, edge_colors) containing:
      * the coordinates of all keypoints of all detected entities;
      * the coordinates of all skeleton edges of all detected entities;
      * the colors in which the edges should be plotted.
  """
  keypoints_all = []
  keypoint_edges_all = []
  edge_colors = []
  num_instances, _, _, _ = keypoints_with_scores.shape
  for idx in range(num_instances):
    kpts_x = keypoints_with_scores[0, idx, :, 1]
    kpts_y = keypoints_with_scores[0, idx, :, 0]
    kpts_scores = keypoints_with_scores[0, idx, :, 2]
    kpts_absolute_xy = np.stack(
        [width * np.array(kpts_x), height * np.array(kpts_y)], axis=-1)
    kpts_above_thresh_absolute = kpts_absolute_xy[
        kpts_scores > keypoint_threshold, :]
    keypoints_all.append(kpts_above_thresh_absolute)

    for edge_pair, color in KEYPOINT_EDGE_INDS_TO_COLOR.items():
      if (kpts_scores[edge_pair[0]] > keypoint_threshold and
          kpts_scores[edge_pair[1]] > keypoint_threshold):
        x_start = kpts_absolute_xy[edge_pair[0], 0]
        y_start = kpts_absolute_xy[edge_pair[0], 1]
        x_end = kpts_absolute_xy[edge_pair[1], 0]
        y_end = kpts_absolute_xy[edge_pair[1], 1]
        line_seg = np.array([[x_start, y_start], [x_end, y_end]])
        keypoint_edges_all.append(line_seg)
        edge_colors.append(color)
  if keypoints_all:
    keypoints_xy = np.concatenate(keypoints_all, axis=0)
  else:
    keypoints_xy = np.zeros((0, 17, 2))

  if keypoint_edges_all:
    edges_xy = np.stack(keypoint_edges_all, axis=0)
  else:
    edges_xy = np.zeros((0, 2, 2))
  return keypoints_xy, edges_xy, edge_colors


def draw_prediction_on_image(
    image, keypoints_with_scores, crop_region=None, close_figure=False,
    output_image_height=None):
  """Draws the keypoint predictions on image.

  Args:
    image: A numpy array with shape [height, width, channel] representing the
      pixel values of the input image.
    keypoints_with_scores: A numpy array with shape [1, 1, 17, 3] representing
      the keypoint coordinates and scores returned from the MoveNet model.
    crop_region: A dictionary that defines the coordinates of the bounding box
      of the crop region in normalized coordinates (see the init_crop_region
      function below for more detail). If provided, this function will also
      draw the bounding box on the image.
    output_image_height: An integer indicating the height of the output image.
      Note that the image aspect ratio will be the same as the input image.

  Returns:
    A numpy array with shape [out_height, out_width, channel] representing the
    image overlaid with keypoint predictions.
  """
  height, width, channel = image.shape
  aspect_ratio = float(width) / height
  fig, ax = plt.subplots(figsize=(12 * aspect_ratio, 12))
  # To remove the huge white borders
  fig.tight_layout(pad=0)
  ax.margins(0)
  ax.set_yticklabels([])
  ax.set_xticklabels([])
  plt.axis('off')

  im = ax.imshow(image)
  line_segments = LineCollection([], linewidths=(4), linestyle='solid')
  ax.add_collection(line_segments)
  # Turn off tick labels
  scat = ax.scatter([], [], s=60, color='#FF1493', zorder=3)

  (keypoint_locs, keypoint_edges,
   edge_colors) = _keypoints_and_edges_for_display(
       keypoints_with_scores, height, width)

  line_segments.set_segments(keypoint_edges)
  line_segments.set_color(edge_colors)
  if keypoint_edges.shape[0]:
    line_segments.set_segments(keypoint_edges)
    line_segments.set_color(edge_colors)
  if keypoint_locs.shape[0]:
    scat.set_offsets(keypoint_locs)

  if crop_region is not None:
    xmin = max(crop_region['x_min'] * width, 0.0)
    ymin = max(crop_region['y_min'] * height, 0.0)
    rec_width = min(crop_region['x_max'], 0.99) * width - xmin
    rec_height = min(crop_region['y_max'], 0.99) * height - ymin
    rect = patches.Rectangle(
        (xmin,ymin),rec_width,rec_height,
        linewidth=1,edgecolor='b',facecolor='none')
    ax.add_patch(rect)

  fig.canvas.draw()
  image_from_plot = np.frombuffer(fig.canvas.buffer_rgba(), dtype=np.uint8)
  image_from_plot = image_from_plot.reshape(
      fig.canvas.get_width_height()[::-1] + (4,))
  image_from_plot = image_from_plot[:,:,:3] # Drop the alpha channel
  plt.close(fig)
  if output_image_height is not None:
    output_image_width = int(output_image_height / height * width)
    image_from_plot = cv2.resize(
        image_from_plot, dsize=(output_image_width, output_image_height),
         interpolation=cv2.INTER_CUBIC)
  return image_from_plot

def to_gif(images, duration):
  """Converts image sequence (4D numpy array) to gif."""
  imageio.mimsave('./animation.gif', images, duration=duration)
  return embed.embed_file('./animation.gif')

def progress(value, max=100):
  return HTML("""
      <progress
          value='{value}'
          max='{max}',
          style='width: 100%'
      >
          {value}
      </progress>
  """.format(value=value, max=max))

# Resize and pad the image to keep the aspect ratio and fit the expected size.
input_image = tf.expand_dims(image, axis=0)
input_image = tf.image.resize_with_pad(input_image, input_size, input_size)

# Run model inference.
keypoints_with_scores = movenet(input_image)

# Visualize the predictions with image.
display_image = tf.expand_dims(image, axis=0)
display_image = tf.cast(tf.image.resize_with_pad(
    display_image, 1280, 1280), dtype=tf.int32)
output_overlay = draw_prediction_on_image(
    np.squeeze(display_image.numpy(), axis=0), keypoints_with_scores)

plt.figure(figsize=(5, 5))
plt.imshow(output_overlay)
_ = plt.axis('off')

# Load the input image.
image_path = 'dance.gif'
image = tf.io.read_file(image_path)
image = tf.image.decode_gif(image)

# Confidence score to determine whether a keypoint prediction is reliable.
MIN_CROP_KEYPOINT_SCORE = 0.2

def init_crop_region(image_height, image_width):
  """Defines the default crop region.

  The function provides the initial crop region (pads the full image from both
  sides to make it a square image) when the algorithm cannot reliably determine
  the crop region from the previous frame.
  """
  if image_width > image_height:
    box_height = image_width / image_height
    box_width = 1.0
    y_min = (image_height / 2 - image_width / 2) / image_height
    x_min = 0.0
  else:
    box_height = 1.0
    box_width = image_height / image_width
    y_min = 0.0
    x_min = (image_width / 2 - image_height / 2) / image_width

  return {
    'y_min': y_min,
    'x_min': x_min,
    'y_max': y_min + box_height,
    'x_max': x_min + box_width,
    'height': box_height,
    'width': box_width
  }

def torso_visible(keypoints):
  """Checks whether there are enough torso keypoints.

  This function checks whether the model is confident at predicting one of the
  shoulders/hips which is required to determine a good crop region.
  """
  return ((keypoints[0, 0, KEYPOINT_DICT['left_hip'], 2] >
           MIN_CROP_KEYPOINT_SCORE or
          keypoints[0, 0, KEYPOINT_DICT['right_hip'], 2] >
           MIN_CROP_KEYPOINT_SCORE) and
          (keypoints[0, 0, KEYPOINT_DICT['left_shoulder'], 2] >
           MIN_CROP_KEYPOINT_SCORE or
          keypoints[0, 0, KEYPOINT_DICT['right_shoulder'], 2] >
           MIN_CROP_KEYPOINT_SCORE))

def determine_torso_and_body_range(
    keypoints, target_keypoints, center_y, center_x):
  """Calculates the maximum distance from each keypoints to the center location.

  The function returns the maximum distances from the two sets of keypoints:
  full 17 keypoints and 4 torso keypoints. The returned information will be
  used to determine the crop size. See determineCropRegion for more detail.
  """
  torso_joints = ['left_shoulder', 'right_shoulder', 'left_hip', 'right_hip']
  max_torso_yrange = 0.0
  max_torso_xrange = 0.0
  for joint in torso_joints:
    dist_y = abs(center_y - target_keypoints[joint][0])
    dist_x = abs(center_x - target_keypoints[joint][1])
    if dist_y > max_torso_yrange:
      max_torso_yrange = dist_y
    if dist_x > max_torso_xrange:
      max_torso_xrange = dist_x

  max_body_yrange = 0.0
  max_body_xrange = 0.0
  for joint in KEYPOINT_DICT.keys():
    if keypoints[0, 0, KEYPOINT_DICT[joint], 2] < MIN_CROP_KEYPOINT_SCORE:
      continue
    dist_y = abs(center_y - target_keypoints[joint][0]);
    dist_x = abs(center_x - target_keypoints[joint][1]);
    if dist_y > max_body_yrange:
      max_body_yrange = dist_y

    if dist_x > max_body_xrange:
      max_body_xrange = dist_x

  return [max_torso_yrange, max_torso_xrange, max_body_yrange, max_body_xrange]

def determine_crop_region(
      keypoints, image_height,
      image_width):
  """Determines the region to crop the image for the model to run inference on.

  The algorithm uses the detected joints from the previous frame to estimate
  the square region that encloses the full body of the target person and
  centers at the midpoint of two hip joints. The crop size is determined by
  the distances between each joints and the center point.
  When the model is not confident with the four torso joint predictions, the
  function returns a default crop which is the full image padded to square.
  """
  target_keypoints = {}
  for joint in KEYPOINT_DICT.keys():
    target_keypoints[joint] = [
      keypoints[0, 0, KEYPOINT_DICT[joint], 0] * image_height,
      keypoints[0, 0, KEYPOINT_DICT[joint], 1] * image_width
    ]

  if torso_visible(keypoints):
    center_y = (target_keypoints['left_hip'][0] +
                target_keypoints['right_hip'][0]) / 2;
    center_x = (target_keypoints['left_hip'][1] +
                target_keypoints['right_hip'][1]) / 2;

    (max_torso_yrange, max_torso_xrange,
      max_body_yrange, max_body_xrange) = determine_torso_and_body_range(
          keypoints, target_keypoints, center_y, center_x)

    crop_length_half = np.amax(
        [max_torso_xrange * 1.9, max_torso_yrange * 1.9,
          max_body_yrange * 1.2, max_body_xrange * 1.2])

    tmp = np.array(
        [center_x, image_width - center_x, center_y, image_height - center_y])
    crop_length_half = np.amin(
        [crop_length_half, np.amax(tmp)]);

    crop_corner = [center_y - crop_length_half, center_x - crop_length_half];

    if crop_length_half > max(image_width, image_height) / 2:
      return init_crop_region(image_height, image_width)
    else:
      crop_length = crop_length_half * 2;
      return {
        'y_min': crop_corner[0] / image_height,
        'x_min': crop_corner[1] / image_width,
        'y_max': (crop_corner[0] + crop_length) / image_height,
        'x_max': (crop_corner[1] + crop_length) / image_width,
        'height': (crop_corner[0] + crop_length) / image_height -
            crop_corner[0] / image_height,
        'width': (crop_corner[1] + crop_length) / image_width -
            crop_corner[1] / image_width
      }
  else:
    return init_crop_region(image_height, image_width)

def crop_and_resize(image, crop_region, crop_size):
  """Crops and resize the image to prepare for the model input."""
  boxes=[[crop_region['y_min'], crop_region['x_min'],
          crop_region['y_max'], crop_region['x_max']]]
  output_image = tf.image.crop_and_resize(
      image, box_indices=[0], boxes=boxes, crop_size=crop_size)
  return output_image

def run_inference(movenet, image, crop_region, crop_size):
  """Runs model inference on the cropped region.

  The function runs the model inference on the cropped region and updates the
  model output to the original image coordinate system.
  """
  image_height, image_width, _ = image.shape
  input_image = crop_and_resize(
    tf.expand_dims(image, axis=0), crop_region, crop_size=crop_size)
  # Run model inference.
  keypoints_with_scores = movenet(input_image)
  # Update the coordinates.
  for idx in range(17):
    keypoints_with_scores[0, 0, idx, 0] = (
        crop_region['y_min'] * image_height +
        crop_region['height'] * image_height *
        keypoints_with_scores[0, 0, idx, 0]) / image_height
    keypoints_with_scores[0, 0, idx, 1] = (
        crop_region['x_min'] * image_width +
        crop_region['width'] * image_width *
        keypoints_with_scores[0, 0, idx, 1]) / image_width
  return keypoints_with_scores

# Load the input image.
num_frames, image_height, image_width, _ = image.shape
crop_region = init_crop_region(image_height, image_width)

output_images = []
bar = display(progress(0, num_frames-1), display_id=True)
for frame_idx in range(num_frames):
  keypoints_with_scores = run_inference(
      movenet, image[frame_idx, :, :, :], crop_region,
      crop_size=[input_size, input_size])
  output_images.append(draw_prediction_on_image(
      image[frame_idx, :, :, :].numpy().astype(np.int32),
      keypoints_with_scores, crop_region=None,
      close_figure=True, output_image_height=300))
  crop_region = determine_crop_region(
      keypoints_with_scores, image_height, image_width)
  bar.update(progress(frame_idx, num_frames-1))

# Prepare gif visualization.
output = np.stack(output_images, axis=0)
to_gif(output, duration=100)



"""HERE ENDS STICK FIGURE PREDICTION. NOW BEGINS HUGGING FACE"""

from datasets import load_dataset

# Login using e.g. `huggingface-cli login` to access this dataset
ds = load_dataset("simplexsigil2/omnifall", "labels")

from datasets import load_dataset
import pandas as pd

# Load the datasets
print("Loading datasets...")

# Note: We separate segment labels and split definitions, but hugginface datasets always expects a split.
# Thats why all labels are in the train split when loaded, but we create the actual splits afterwards.
labels = load_dataset("simplexsigil2/omnifall", "labels")["train"]

cv_split = load_dataset("simplexsigil2/omnifall", "cv")
cs_split = load_dataset("simplexsigil2/omnifall", "cs")

# There are many more splits, relevant for the paper:
# - cv-staged -> Only lab datasets
# - cs-staged -> Only lab datasets
# - cv-staged-wild -> Lab datasets for train and val, only OOPS-Fall in test set
# - cs-staged-wild -> Lab datasets for train and val, only OOPS-Fall in test set

# Convert to pandas DataFrames
labels_df = pd.DataFrame(labels)
print(f"Labels dataframe shape: {labels_df.shape}")

# Process each split type (CV and CS)
for split_name, split_data in [("CV", cv_split), ("CS", cs_split)]:
    print(f"\n{split_name} Split Processing:")

    # Process each split (train, validation, test)
    for subset_name, subset in split_data.items():
        # Convert to DataFrame
        subset_df = pd.DataFrame(subset)

        # Join with labels on 'path'
        merged_df = pd.merge(subset_df, labels_df, on="path", how="left")

        # Print statistics
        print(f"  {subset_name} split: {len(subset_df)} videos, {merged_df.dropna().shape[0]} labelled segments")

        # Print examples
        if not merged_df.empty:
            print(f"\n  {subset_name.upper()} EXAMPLES:")
            random_samples = merged_df.sample(min(3, len(merged_df)))
            for i, (_, row) in enumerate(random_samples.iterrows()):
                print(f"  Example {i+1}:")
                print(f"    Path: {row['path']}")
                print(f"    Start: {row['start']}")
                print(f"    End: {row['end']}")
                print(f"    Label: {row['label']}")
                print(f"    Subject: {row['subject']}")
                print(f"    Dataset: {row['dataset']}")
                print(f"    Camera: {row['cam']}")
                print()

merged_df

filtered_df = merged_df[merged_df["dataset"] == "up_fall"]
subjects_to_keep = [f"Subject{i}" for i in range(1, 7)]

filtered_df = filtered_df[filtered_df["path"].apply(lambda x: x.split("/")[0] in subjects_to_keep)]
filtered_df

df = filtered_df

combine_df = pd.read_csv("custom_data.csv")
combine_df["dataset"] = "CUSTOM"
df = pd.concat([df, combine_df], ignore_index = True)

df["path"] = "images/" + df["path"] + ".mp4"
df

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
import zipfile
from io import BytesIO
import os
import cv2
import numpy as np

"""Data Usage"""

def createFolder(directory):
    try:
        if not os.path.exists(directory):
            os.makedirs(directory)
    except OSError:
        print('An error ocurred while creating direcory: "' + directory + '"')


def fileFinder(f_name, p_id, drive):
    f_id = ''
    file_list = drive.ListFile({'q': "'" + p_id + "' in parents and trashed=false"}).GetList()
    for file in file_list:
        if file['title'] == f_name:
            f_id = file['id']
            break
    return f_id

def extract_number(image_file):
    """
    Extracts the numeric part from the filename like:
    'prefix_something_123.png' -> 123.0
    """
    # Take the last underscore-separated part
    last_part = image_file.split("_")[-1]
    # Remove the extension
    number_str = os.path.splitext(last_part)[0]
    # Convert to float
    return float(number_str)


# end of fileFinder

# A function that Downloads files from Google Drive
# f_name : file name, f_id : file id, path : directory in which to save the file, drive : the drive object being used

def fileDownloader(f_name, f_id, path, drive, unzip=True):
    try:
        file_d = drive.CreateFile({'id': f_id})

        if f_name.endswith('.zip') and unzip:
            # Download zip file to memory and extract it
            print(f'  Downloading and extracting {f_name}...')

            # Method 1: Download to memory and extract
            file_d.FetchContent()
            zip_content = BytesIO(file_d.content.getvalue())

            with zipfile.ZipFile(zip_content, 'r') as zip_ref:

                  image_files = sorted([f for f in zip_ref.namelist() if f.endswith(('.png', '.jpg', '.jpeg'))])
                  if not image_files:
                    pass
                  fps = 20

                  first_img_data = zip_ref.read(image_files[0])
                  first_img_array = np.frombuffer(first_img_data, np.uint8)
                  frame = cv2.imdecode(first_img_array, cv2.IMREAD_COLOR)
                  height, width, layers = frame.shape

                  # Set up video writer
                  out = cv2.VideoWriter(path + "/" + f"{f_name[:-4]}.mp4",
                    cv2.VideoWriter_fourcc(*'mp4v'),
                    fps,
                    (width, height))

                  # Loop through images and write to video
                  for img_name in image_files:
                    img_data = zip_ref.read(img_name)
                    img_array = np.frombuffer(img_data, np.uint8)
                    img = cv2.imdecode(img_array, cv2.IMREAD_COLOR)
                    out.write(img)
                  out.release()

        elif f_name.endswith('.zip') and not unzip:
            # Just download the zip file without extracting
            print(f'  Downloading {f_name} (without extracting)...')
            file_d.GetContentFile(path + f_name)

        else:
            # For non-zip files, download normally
            print(f'  Downloading {f_name}...')
            file_d.GetContentFile(path + f_name)
    except zipfile.BadZipFile:
        print(f'  Error: {f_name} is not a valid zip file. Downloading without extraction...')
        file_d.GetContentFile(path + f_name)
    except Exception as e:
        print(f'  Error downloading/extracting {f_name}: {str(e)}')
        raise


# end of fileDownloader
def connect():
    gauth = GoogleAuth()
    # Creates local webserver and auto handles authentication.
    gauth.LocalWebserverAuth()
    # Reads stored credentials (if any) to avoid opening the browser
    gauth.LoadCredentialsFile('credentials.json')
    drive = GoogleDrive(gauth)
    return gauth, drive


# A function to generate, refresh or authenticate Google Drive credentials
def refresh_gauth(gauth, drive):
    flg = False
    try:
        if gauth.credentials is None:
            gauth.LocalWebserverAuth()
        elif gauth.access_token_expired:
            gauth.Refresh()
        else:
            gauth.Authorize()
        gauth.SaveCredentialsFile('credentials.json')
        drive = GoogleDrive(gauth)
    except Exception as e:
        print('An error ocurred: ' + str(e))
        flg = True
    return gauth, drive, flg


# A function that handles downloads
def download(path, f_name, p_id, gauth, drive):
    gauth, drive, v_flg = refresh_gauth(gauth, drive)
    if v_flg:
        return gauth, drive, True
    f_id = fileFinder(f_name, p_id, drive)
    gauth, drive, v_num = refresh_gauth(gauth, drive)
    if v_flg:
        return gauth, drive, True
    print('--------Downloading:' + f_name)
    fileDownloader(f_name, f_id, path, drive)
    print('----------Download complete')
    return gauth, drive, False


# A function to handle feature downloads
def featureDownload(gral='',
                    n_sub=[1, 17],
                    n_act=[1, 11],
                    n_trl=[1, 3],
                    t_window=['1&0.5', '2&1', '3&1.5'],
                    csv_files=True,
                    cameras=True,
                    feat_cam_OF=True,
                    Complete_OF=False,
                    n_cam=[1, 2]):
    gauth, drive = connect()
    # A flag to ensure that a connection with Google Drive is made
    v_flg = False
    for i in range(n_sub[0], n_sub[1] + 1):
        sub = 'Subject' + str(i)
        print('--' + sub)
        for j in range(n_act[0], n_act[1] + 1):
            act = 'Activity' + str(j)
            print('S' + str(i) + '--' + act)
            for k in range(n_trl[0], n_trl[1] + 1):
                trl = 'Trial' + str(k)
                print('S' + str(i) + '-A' + str(j) + '--' + trl)
                path = gral + sub + '//' + act + '//' + trl + '//'
                createFolder(path)
                # Resized camera OF csv
                if cameras:
                    f_name = sub + act + trl + 'CameraResizedOF.csv'
                    gauth, drive, v_flg = refresh_gauth(gauth, drive)
                    if v_flg:
                        break
                    s_id = fileFinder(sub, '1XDJELfyqXSgjQg-z-s5MHG_YxsSZO5vS', drive)
                    a_id = fileFinder(act, s_id, drive)
                    gauth, drive, v_flg = download(path, f_name, a_id, gauth, drive)

    if v_flg:
        print('An error ocurred while connecting to Google Drive')


def dataBaseDownload(gral='',
                     n_sub=[1, 17],
                     n_act=[1, 11],
                     n_trl=[1, 3],
                     csv_files=True,
                     cameras=True,
                     n_cam=[1, 2]):
    gauth, drive = connect()
    v_flg = False
    # parent folder´s id
    p_id = '1AItqj3Ue-iv7NSdR7Qta1Ez4spRjCo58'
    for i in range(n_sub[0], n_sub[1] + 1):
        if i < 4:
          continue

        sub = 'Subject' + str(i)

        print('--' + sub)
        gauth, drive, v_flg = refresh_gauth(gauth, drive)
        if v_flg:
            break
        s_id = fileFinder(sub, p_id, drive)
        if s_id == '':
            print('The folder "' + sub + '" could not be found!')
            v_flg = True
            break
        for j in range(n_act[0], n_act[1] + 1):
            act = 'Activity' + str(j)
            print('S' + str(i) + '--' + act)
            gauth, drive, v_flg = refresh_gauth(gauth, drive)
            if v_flg:
                break
            a_id = fileFinder(act, s_id, drive)
            for k in range(n_trl[0], n_trl[1] + 1):
                if v_flg:
                    break
                trl = 'Trial' + str(k)
                print('S' + str(i) + '-A' + str(j) + '--' + trl)
                path = gral + sub + '//' + act + '//' + trl + '//'
                createFolder(path)
                f_name = sub + act + trl + '.csv'
                gauth, drive, v_flg = refresh_gauth(gauth, drive)
                t_id = fileFinder(trl, a_id, drive)
                if v_flg:
                    break
                if (cameras):
                    for l in range(n_cam[0], n_cam[1] + 1):
                        cam = 'Camera' + str(l)
                        f_name = sub + act + trl + cam + '.zip'
                        gauth, drive, v_flg = download(path, f_name, t_id, gauth, drive)
    if v_flg:
        print('An error ocurred while connecting to Google Drive')

dataBaseDownload('ParentFolder//', csv_files=False, cameras=True)

"""MODEL CREATION BEGINS HERE"""

import tqdm
import random
import pathlib
import itertools
import collections

import cv2
import einops
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

import tensorflow as tf
import keras
from keras import layers

def split_train_val(df, train_frac = 0.8, seed = 10):
    if seed is not None:
        np.random.seed(seed)

    train_list = []
    val_list = []

    # iterate over each label
    for label, group in df.groupby("label"):
        n = len(group)
        # shuffle indices
        shuffled = group.sample(frac=1, random_state=seed)
        split_idx = int(n * train_frac)
        train_list.append(shuffled.iloc[:split_idx])
        val_list.append(shuffled.iloc[split_idx:])

    # concatenate all labels back
    train_df = pd.concat(train_list).sample(frac=1, random_state=seed).reset_index(drop=True)
    val_df = pd.concat(val_list).sample(frac=1, random_state=seed).reset_index(drop=True)

    return train_df, val_df
def format_frames(frame, output_size):
  """
    Pad and resize an image from a video.

    Args:
      frame: Image that needs to resized and padded.
      output_size: Pixel size of the output frame image.

    Return:
      Formatted frame with padding of specified output size.
  """
  frame = tf.image.convert_image_dtype(frame, tf.float32)
  frame = tf.image.resize_with_pad(frame, *output_size)
  return frame


def generate_sequence(start, end, average_dist, n_elements, jitter=0.4):
    """
    Generate a random increasing sequence of integers.

    Args:
        start (int): Minimum starting value.
        end (int): Maximum ending value.
        average_dist (float): Desired average spacing between values.
        jitter (float): Fractional randomness to add to spacing (default 0.2 = ±20%).

    Returns:
        list[int]: Random increasing sequence.
    """
    if n_elements == 1:
        return [start]

    sequence = [start]

    for i in range(n_elements - 1):
        # Determine remaining steps
        remaining_steps = n_elements - len(sequence)
        remaining_distance = end - sequence[-1]

        # Target step based on average_dist, but limited by remaining distance
        target_step = min(average_dist, remaining_distance / remaining_steps)

        # Add jitter
        step = int(random.uniform(target_step * (1 - jitter), target_step * (1 + jitter)))
        step = max(1, step)  # ensure strictly increasing

        next_val = sequence[-1] + step

        # Ensure last element <= end
        if len(sequence) == n_elements - 1:
            next_val = min(next_val, end)

        sequence.append(next_val)

    return sequence


def frames_from_video_file(video_path, time_stamp, n_frames, output_size = (224,224)):
    """
    Creates frames from each video file present for each category.

    Args:
      video_path: File path to the video.
      n_frames: Number of frames to be created per video file.
      output_size: Pixel size of the output frame image.

    Return:
      An NumPy array of frames in the shape of (n_frames, height, width, channels).
    """
    # Read each video frame by frame
    result = []
    src = cv2.VideoCapture(str(video_path))

    # all videos are 20 FPS
    video_length = round((time_stamp[1] - time_stamp[0]) * 20)

    average_distance = video_length / n_frames

    start_frame = round(time_stamp[0]*20)
    end_frame = round(time_stamp[1]*20)

    frame_indexes = generate_sequence(start_frame, end_frame, average_distance, n_frames)


    for index in frame_indexes:
        src.set(cv2.CAP_PROP_POS_FRAMES, index)
        ret, frame = src.read()
        if ret:
            frame = format_frames(frame, output_size)
            result.append(frame)
        else:
            result.append(np.zeros_like(result[0]))
    src.release()

    result = np.array(result)[..., [2, 1, 0]]
    return result

def preprocess_and_save_videos(df, output_dir="preprocessed_videos"):
    os.makedirs(output_dir, exist_ok=True)

    for idx, row in tqdm.tqdm(df.iterrows(), total=len(df)):
        video_path = row['path']
        time_stamp = (row['start'], row['end'])
        label = row['label']

        # Extract frames
        frames = frames_from_video_file(video_path, time_stamp, n_frames=20)
        save_path = os.path.join(output_dir, f"video_{idx}.npz")
        np.savez_compressed(save_path, frames=frames, label=label)

        # Update dataframe with new path
        df.at[idx, 'preprocessed_path'] = save_path

    return df

class FrameGenerator:
  def __init__(self, df, training = False):
    """ Returns a set of frames with their associated label.

      Args:
        path: Video file paths.
        training: Boolean to determine if training dataset is being created.
    """
    self.df = df
    self.training = training
    self.class_id = sorted(set(df["label"]))

  def get_files_and_meta(self):
    video_paths = self.df["path"]
    time_stamps = (self.df["start"], self.df["end"])
    labels = self.df["label"]

    return video_paths, time_stamps, labels

  def __call__(self):
    video_paths, time_stamps, labels = self.get_files_and_meta()

    datapoint = list(zip(video_paths, time_stamps, labels))

    if self.training:
        random.shuffle(datapoint)

    for path, time_stamp, label in datapoint:
        video_frames = frames_from_video_file(path, time_stamp, n_frames = 20)
        yield video_frames, label

class FastFrameGenerator:
    def __init__(self, df, training=False):
        self.df = df
        self.training = training

    def __call__(self):
        paths = self.df['preprocessed_path'].values
        if self.training:
            np.random.shuffle(paths)

        for path in paths:
            data = np.load(path)
            yield data['frames'], data['label']

import os

train_df, val_df = split_train_val(df)

print("Preprocessing training videos...")
train_df = preprocess_and_save_videos(train_df, "train_preprocessed")

print("Preprocessing validation videos...")
val_df = preprocess_and_save_videos(val_df, "val_preprocessed")

import tensorflow_hub as hub
import keras

batch_size = 8

output_signature = (
    tf.TensorSpec(shape=(20, 224, 224, 3), dtype=tf.float32),
    tf.TensorSpec(shape=(), dtype=tf.int16)
)

train_ds = tf.data.Dataset.from_generator(
    FastFrameGenerator(train_df, training=True),
    output_signature=output_signature
)

# Optimize training pipeline
train_ds = (train_ds
    .batch(batch_size)
    .prefetch(tf.data.AUTOTUNE)
    .repeat()  # Infinite dataset
)

# Create validation dataset
val_ds = tf.data.Dataset.from_generator(
    FastFrameGenerator(val_df, training=False),
    output_signature=output_signature
)

# Optimize validation pipeline
val_ds = (val_ds
    .batch(batch_size)
    .prefetch(tf.data.AUTOTUNE)
)

print(f"\nTraining samples: {len(train_df)}")
print(f"Validation samples: {len(val_df)}")

print("Number of training videos:", len(train_df))
print("Number of validation videos:", len(val_df))

gen = FrameGenerator(train_df, training=True)
# get first batch
for video_frames, label in gen():
    print("video_frames shape:", video_frames.shape)
    print("label:", label)
    break

HEIGHT = 224
WIDTH = 224

class Conv2Plus1D(keras.layers.Layer):
  def __init__(self, filters, kernel_size, padding):
    """
      A sequence of convolutional layers that first apply the convolution operation over the
      spatial dimensions, and then the temporal dimension.
    """
    super().__init__()
    self.seq = keras.Sequential([
        # Spatial decomposition
        layers.Conv3D(filters=filters,
                      kernel_size=(1, kernel_size[1], kernel_size[2]),
                      padding=padding),
        # Temporal decomposition
        layers.Conv3D(filters=filters,
                      kernel_size=(kernel_size[0], 1, 1),
                      padding=padding)
        ])

  def call(self, x):
    return self.seq(x)

class ResidualMain(keras.layers.Layer):
  """
    Residual block of the model with convolution, layer normalization, and the
    activation function, ReLU.
  """
  def __init__(self, filters, kernel_size):
    super().__init__()
    self.seq = keras.Sequential([
        Conv2Plus1D(filters=filters,
                    kernel_size=kernel_size,
                    padding='same'),
        layers.LayerNormalization(),
        layers.ReLU(),
        Conv2Plus1D(filters=filters,
                    kernel_size=kernel_size,
                    padding='same'),
        layers.LayerNormalization()
    ])

  def call(self, x):
    return self.seq(x)

class Project(keras.layers.Layer):
  """
    Project certain dimensions of the tensor as the data is passed through different
    sized filters and downsampled.
  """
  def __init__(self, units):
    super().__init__()
    self.seq = keras.Sequential([
        layers.Dense(units),
        layers.LayerNormalization()
    ])

  def call(self, x):
    return self.seq(x)

def add_residual_block(input, filters, kernel_size):
  """
    Add residual blocks to the model. If the last dimensions of the input data
    and filter size does not match, project it such that last dimension matches.
  """
  out = ResidualMain(filters,
                     kernel_size)(input)

  res = input
  if out.shape[-1] != input.shape[-1]:
    res = Project(out.shape[-1])(res)

  return layers.add([res, out])

class ResizeVideo(keras.layers.Layer):
  def __init__(self, height, width):
    super().__init__()
    self.height = height
    self.width = width
    self.resizing_layer = layers.Resizing(self.height, self.width)

  def call(self, video):
    """
      Use the einops library to resize the tensor.

      Args:
        video: Tensor representation of the video, in the form of a set of frames.

      Return:
        A downsampled size of the video according to the new height and width it should be resized to.
    """
    # b stands for batch size, t stands for time, h stands for height,
    # w stands for width, and c stands for the number of channels.
    old_shape = einops.parse_shape(video, 'b t h w c')
    images = einops.rearrange(video, 'b t h w c -> (b t) h w c')
    images = self.resizing_layer(images)
    videos = einops.rearrange(
        images, '(b t) h w c -> b t h w c',
        t = old_shape['t'])
    return videos

input_shape = (None, 20, HEIGHT, WIDTH, 3)
input = layers.Input(shape=(input_shape[1:]))
x = input

x = Conv2Plus1D(filters=16, kernel_size=(3, 7, 7), padding='same')(x)
x = layers.BatchNormalization()(x)
x = layers.ReLU()(x)
x = ResizeVideo(HEIGHT // 2, WIDTH // 2)(x)

# Block 1
x = add_residual_block(x, 16, (3, 3, 3))
x = ResizeVideo(HEIGHT // 4, WIDTH // 4)(x)

# Block 2
x = add_residual_block(x, 32, (3, 3, 3))
x = ResizeVideo(HEIGHT // 8, WIDTH // 8)(x)

# Block 3
x = add_residual_block(x, 64, (3, 3, 3))
x = ResizeVideo(HEIGHT // 16, WIDTH // 16)(x)

# Block 4
x = add_residual_block(x, 128, (3, 3, 3))

x = layers.GlobalAveragePooling3D()(x)
x = layers.Flatten()(x)
x = layers.Dense(10)(x)

model = keras.Model(input, x)
model.summary()

frames, label = next(iter(train_ds))
model.build(frames)

model.compile(loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              optimizer = keras.optimizers.Adam(learning_rate = 0.0001),
              metrics = ['accuracy'])

from tensorflow.keras.callbacks import ModelCheckpoint

# Create a callback that saves the model after every epoch
checkpoint_cb = ModelCheckpoint(
    "model_checkpoint.keras",       # file path to save the model
    save_best_only=True,         # only save the model if validation loss improves
    monitor="val_loss",          # metric to monitor
    mode="min",                  # save when val_loss is minimized
    verbose=1
)

history = model.fit(x = train_ds,
                    epochs = 200,
                    validation_data = val_ds,
                    steps_per_epoch=len(train_df)//16,
                    callbacks=[checkpoint_cb])

model.save("SentiVision.keras")

model.predict(val_ds)

import gc
gc.collect()

